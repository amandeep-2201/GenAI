{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart with RAGStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to set up a simple RAG pipeline with RAGStack. At the end of this notebook, you will have a fully functioning Question/Answer model that can answer questions using your supplied documents.\n",
    "\n",
    "A RAG pipeline requires, at minimum, a vector store, an embedding model, and an LLM. In this tutorial, you will use an Astra DB vector store, an OpenAI embedding model, an OpenAI LLM, and LangChain to orchestrate it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need a vector-enabled Astra database and an OpenAI Account.\n",
    "\n",
    "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "* Create an [OpenAI account](https://openai.com/)\n",
    "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "* Get your Astra DB Endpoint:\n",
    "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "`ragstack-ai` includes all the packages you need to build a RAG pipeline.\n",
    "\n",
    "`datasets` is used to import a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ragstack-ai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass  #used to take input from user\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: Check if the environment variable is loaded\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))  # Should print the key if it's loaded properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG Pipeline\n",
    "\n",
    "**Embedding Model and Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astra vector store configured\n"
     ]
    }
   ],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "\n",
    "#configure your embedding model and vector store\n",
    "embedding = OpenAIEmbeddings()\n",
    "vstore = AstraDBVectorStore(\n",
    "    collection_name=\"test\",\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    ")\n",
    "print(\"Astra vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example entry : \n",
      "{'author': 'aristotle', 'quote': \"True happiness comes from gaining insight and growing into your best possible self. Otherwise all you're having is immediate gratification pleasure, which is fleeting and doesn't grow you as a person.\", 'tags': 'knowledge'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#load a sample datastax dataset from hugging face\n",
    "\n",
    "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
    "print(\"An example entry : \")\n",
    "print(philo_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/datastax/philosopher-quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "#Constructs a set of documents from your data. Documents can be used as inputs to your vectore store.\n",
    "docs = []\n",
    "\n",
    "#Each entry is expected to be a dictionary with keys for author, tags, and quote.\n",
    "for entry in philo_dataset:\n",
    "    metadata = {\"author\": entry[\"author\"]}\n",
    "    if entry[\"tags\"]:\n",
    "        #add metadata tags to the metadata dictionary\n",
    "        for tag in entry[\"tags\"].split(\";\"):\n",
    "            metadata[tag]= \"y\"\n",
    "    # Create a langchain document with the qoute and metadata tags\n",
    "    doc = Document(page_content = entry[\"quote\"], metadata=metadata)\n",
    "    docs.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'author': 'kant', 'knowledge': 'y'}, page_content='The schematicism by which our understanding deals with the phenomenal world ... is a skill so deeply hidden in the human soul that we shall hardly guess the secret trick that Nature here employs.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserted 450 documents.\n"
     ]
    }
   ],
   "source": [
    "# Create embedddings by inserting your documents into the vector store.\n",
    "inserted_ids = vstore.add_documents(docs)\n",
    "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'documents': [{'_id': '8276b2a06c574e3283911956bb90c1cc', 'content': \"If all I asked was not a great deal, that's my problem!\", 'metadata': {'author': 'sartre', 'ethics': 'y', 'education': 'y'}}, {'_id': '85be77097acc4210a55dba79c3ff904d', 'content': 'Whatever we learn to do, we learn by actually doing it; men come to be builders, for instance, by building, and harp players by playing the harp. In the same way, by doing just acts we come to be just; by doing self-controlled acts, we come to be self-controlled ; and by doing brave acts, we become brave.', 'metadata': {'author': 'aristotle', 'education': 'y', 'knowledge': 'y'}}, {'_id': 'f0c4753b3c1a4ebead226c8c7e383505', 'content': 'Let unswerving integrity be your watchword.', 'metadata': {'author': 'spinoza', 'ethics': 'y'}}, {'_id': 'f11fea2f81194d478a039de6422eb3c8', 'content': 'The death of dogma is the birth of morality.', 'metadata': {'author': 'kant', 'ethics': 'y'}}, {'_id': 'a21238706a2746ba8c61905e27267c6e', 'content': 'In a word, man must create his own essence: it is in throwing himself into the world, suffering there, struggling there, that he gradually defines himself.', 'metadata': {'author': 'sartre', 'ethics': 'y'}}, {'_id': '68b40d8b6fd34cde8bcfa0651f6af4b9', 'content': 'However, for the man who studies to gain insight, books and studies are merely rungs of the ladder on which he climbs to the summit of knowledge. As soon as a rung has raised him up one step, he leaves it behind. On the other hand, the many who study in order to fill their memory do not use the rungs of the ladder for climbing, but take them off and load themselves with them to take away, rejoicing at the increasing weight of the burden. They remain below forever, because they bear what should have bourne them.', 'metadata': {'author': 'schopenhauer', 'knowledge': 'y', 'education': 'y'}}, {'_id': '844354bff15347148d99c968c37d3ee1', 'content': 'World history is a court of judgment.', 'metadata': {'author': 'hegel', 'history': 'y'}}, {'_id': '16c2c2117c534b81b4667c01bc618b9d', 'content': 'He who obeys, does not listen to himself!', 'metadata': {'author': 'nietzsche', 'ethics': 'y'}}, {'_id': 'faf3e8a549ea41b1a9aea6c1468a828e', 'content': \"A man's happiness consists in the free exercise of his highest faculties.\", 'metadata': {'author': 'aristotle', 'knowledge': 'y', 'ethics': 'y', 'education': 'y'}}, {'_id': 'f5d52b9fc236495bb8d0a7027e865578', 'content': 'Thus, the task is not so much to see what no one yet has seen, but to think what nobody yet has thought about that which everybody sees.', 'metadata': {'author': 'schopenhauer', 'knowledge': 'y'}}, {'_id': '5f6e1e3c62cd45f7bc5ce5b8b3719260', 'content': 'Do not train children to learning by force and harshness, but direct them to it by what amuses their minds.', 'metadata': {'author': 'plato', 'ethics': 'y'}}, {'_id': 'c6e20221adf74259ab19892ef8bda2ca', 'content': 'Not all men are worthy of love.', 'metadata': {'author': 'freud'}}, {'_id': '9bd7d5a25ff4453d83e7ce74a1ebe8cf', 'content': 'In revenge and in love, woman is more barbarous than man.', 'metadata': {'author': 'nietzsche'}}, {'_id': 'daccc42de8e6489baf837eae9af74656', 'content': 'To make abstractions hold in reality is to destroy reality.', 'metadata': {'author': 'hegel'}}, {'_id': '72394bca12874b02bb2bdfacc45e50f2', 'content': 'My body and my will are one.', 'metadata': {'author': 'schopenhauer'}}, {'_id': '3c701df8bfe84254a9a02bae714f9282', 'content': 'One of the penalties for refusing to participate in politics is that you end up being governed by your inferiors.', 'metadata': {'author': 'plato', 'politics': 'y'}}, {'_id': '985458e60ae049b29e41db0eaa10e2b8', 'content': 'slipped out of the world, somewhere else like the soul of a dead man. Perhaps he was only a dream...God is dead.', 'metadata': {'author': 'sartre'}}, {'_id': 'd373412383494ebeb915c85ccb19e1d2', 'content': 'Moral Teleology supplies the deficiency in physical Teleology , and first establishes a Theology ; because the latter, if it did not borrow from the former without being observed, but were to proceed consistently, could only found a Demonology , which is incapable of any definite concept.', 'metadata': {'author': 'kant'}}, {'_id': '6a80578feb01465f9df8550f2bab2ddb', 'content': \"I've never known any trouble than an hour's reading didn't assuage.\", 'metadata': {'author': 'schopenhauer'}}, {'_id': 'f914ae86e3ae4e6ea2f27ae6592b3221', 'content': 'Enlightenment is the liberation of man from his self-caused state of minority... Supere aude! Dare to use your own understanding!is thus the motto of the Enlightenment.', 'metadata': {'author': 'kant', 'knowledge': 'y'}}], 'nextPageState': 'KQAAAAEBAAAAIGY5MTRhZTg2ZTNhZTRlNmVhMmYyN2FlNjU5MmIzMjIxAPB////rAA=='}}\n"
     ]
    }
   ],
   "source": [
    "#Checks your collection to verify the documents are embedded.\n",
    "print(vstore.astra_db.collection(\"test\").find())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Basic Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrieve context from your vector database, and pass it to the model with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JN\\LANGCHAIN\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retiever = vstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the questions bases only on the supplied context. If you don't know the answer,  say \"I don't know\".\n",
    "Context: {context},\n",
    "Question: {question}\n",
    "Your answer : \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retiever, \"question\": RunnablePassthrough()}\n",
    "     | prompt\n",
    "     | model\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Philosophers are most concerned with knowledge and truth.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"in the given context, what subject are philosophers most concerned with?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the full measure of sleep which is required to restore it'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"in the given context, what is the most important to allow the brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will delete the collection and all documents in the collection\n",
    "vstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a fully functioning RAG pipeline! Note that there are several different ways to accomplish this, depending on your input data format, vector store, embedding, model, output type, and more. There are also more advanced RAG techniques that leverage new ingestion, retrieval, and generation patterns.\n",
    "\n",
    "RAG is a powerful solution used in tandem with the capabilities of LLMs. Check out our other examples for ideas on how you can build innovative solutions using RAGStack!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
